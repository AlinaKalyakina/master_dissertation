{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Эксперименты по магистерской диссертации\n",
    "Тема - обнаружение инсайдеров  \n",
    "Датасеты - CERT 4.2, 6.2, в виде БД\n",
    "### План работы:  \n",
    "0) Привести набор данных к удобному виду  \n",
    "1) Предобработка данных (генерация \"предложений поведения пользователя\")  \n",
    "2) Предобработка контентных данных. Пока только письма  \n",
    "3) Обучение трансформера типа Bert поведению пользователей  \n",
    "4) Обучение классификатора пользователей  \n",
    "5) Скор аномальности как ошибка классификатора  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "import csv\n",
    "import logging\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "\n",
    "class Device:\n",
    "#     id,date,user,pc,activity\n",
    "    col = {name:idx for idx, name in enumerate([\"id\",\"date\",\"user\",\"pc\",\"activity\"])}\n",
    "    connect = 1\n",
    "    disconnect = 0\n",
    "    feature_len = 2\n",
    "    \n",
    "class Email:\n",
    "#     id,date,user,pc,to,cc,bcc,from,size,attachments,content\n",
    "    col = {name:idx for idx, name in enumerate([\"id\",\"date\",\"user\",\"pc\",\"to\",\"cc\",\"bcc\",\"from\",\"size\",\"attachments\",\"content\"])}\n",
    "    feature_len = 3\n",
    "    \n",
    "class File:\n",
    "#     id,date,user,pc,filename,content\n",
    "    col = {name:idx for idx, name in enumerate([\"id\",\"date\",\"user\",\"pc\",\"filename\",\"content\"])}\n",
    "    feature_len = 1\n",
    "    \n",
    "    \n",
    "class Http:\n",
    "#     id,date,user,pc,url,content\n",
    "    col = {name:idx for idx, name in enumerate([\"id\",\"date\",\"user\",\"pc\",\"url\",\"content\"])}\n",
    "    feature_len = 1\n",
    "    \n",
    "    \n",
    "class Logon:\n",
    "#     id, date, user, pc, activity\n",
    "    col = {name:idx for idx, name in enumerate([\"id\",\"date\",\"user\",\"pc\",\"activity\"])}\n",
    "    logon = 1\n",
    "    logout = 0\n",
    "    feature_len = 2\n",
    "    \n",
    "features_num = 0\n",
    "for c in [Device, Email, File, Http, Logon]:\n",
    "    c.feature_shift = features_num\n",
    "    features_num += c.feature_len + 2\n",
    "    \n",
    "ds_dir =  r\"D:\\simplified_ds\"\n",
    "answers_dir = r\"D:\\answers\"\n",
    "\n",
    "vectorizer_filename = r\"D:\\vectorizer.pkl\"\n",
    "\n",
    "user_list_subdir = r\"LDAP\"\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "File.feature_shift\n",
    "user_computer = {'AAP0352':{6708}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Привести набор данных к удобному виду\n",
    "Пусть каждому пользователю отвечает директория, а файлы data_lig_type в ней отвечают логу пользователя в конкретный день"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = r\"D:\\simplified_ds\"\n",
    "in_dir = r\"D:\\r4.2\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "http_path = os.path.join(in_dir, \"http.csv\")\n",
    "with open(http_path, \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    print(next(reader))\n",
    "    print(next(reader))\n",
    "    print(next(reader))\n",
    "    print(next(reader))\n",
    "    print(next(reader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# id,date,user,pc\n",
    "def parse_file(filename, function):\n",
    "    log_type = os.path.splitext(os.path.basename(filename))[0]\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        next(reader, None)\n",
    "        user_records = dict()\n",
    "        date = ''\n",
    "        for idx, row in enumerate(reader):\n",
    "            user = row[2]\n",
    "            cur_date = datetime.strptime(row[1], '%m/%d/%Y %H:%M:%S').strftime('%Y_%m_%d')\n",
    "            function(row)\n",
    "            if cur_date != date:\n",
    "                for u in user_records:\n",
    "                    with open(os.path.join(out_dir, u, date +\"_\"+ log_type), \"w\") as small_f:\n",
    "                        small_f.write('\\n'.join(user_records[u]))\n",
    "                date = cur_date\n",
    "                user_records = dict()\n",
    "                if date[-1] == '0':\n",
    "                    logging.info(f\"{date} parsed\")\n",
    "                \n",
    "            user_records.setdefault(user, [])\n",
    "            user_records[user].append(','.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_f(row):\n",
    "    row[4] = str(Device.connect if row[4] == \"Connect\" else Device.disconnect)\n",
    "    \n",
    "def email_f(row):\n",
    "    pass\n",
    "    \n",
    "def file_f(row):\n",
    "    pass\n",
    "    \n",
    "def http_f(row):\n",
    "    pass\n",
    "    \n",
    "def logon_f(row):\n",
    "    row[4] = str(Logon.logon if row[4] == \"Logon\" else Logon.logout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим поддиректории c именами работников, а заодно словари \"пользоваель:имеил\" и \"имеил:пользователь\" для всех месяцев\n",
    "email_user_dict = dict()\n",
    "user_email_dict = dict()\n",
    "user_list_dir = os.path.join(in_dir, user_list_subdir)\n",
    "for filename in os.listdir(user_list_dir):\n",
    "    cur_email_user = dict()\n",
    "    cut_user_email = dict()\n",
    "    \n",
    "    with open(os.path.join(user_list_dir, filename), \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        next(reader, None)\n",
    "        for idx, row in enumerate(reader):\n",
    "            user_id = row[1]\n",
    "            user_email = row[2]\n",
    "            os.makedirs(os.path.join(out_dir, user_id), exist_ok=True)\n",
    "            cur_email_user[user_email] = user_id\n",
    "            user_email_dict[user_id] = user_email\n",
    "            \n",
    "    email_user_dict[filename] = cur_email_user\n",
    "    user_email_dict[filename] = cut_user_email\n",
    "    logging.info(f\"{filename} processed\")\n",
    "\n",
    "all_emails = set().union(*[set(email_user_dict[month].keys()) for month in email_user_dict])\n",
    "\n",
    "\n",
    "select_biggest = lambda x: max(x.items(), key=lambda y: y[1])[0]\n",
    "# соберем какие компьютеры личные у каких пользователей\n",
    "#     id,date,user,pc\n",
    "username_pc_dict = {}\n",
    "for username in os.listdir(out_dir):\n",
    "    pc_dict = dict()\n",
    "    user_dir = os.path.join(out_dir, username)\n",
    "    for filename in os.listdir(user_dir):\n",
    "        with open(os.path.join(user_dir, filename), \"r\") as f:   \n",
    "            reader = csv.reader(f, delimiter=',')\n",
    "            pc = row[3]\n",
    "            pc_dict[pc] = pc_dict.get(pc, 0) + 1\n",
    "            \n",
    "    if len(pc_dict) > 1:\n",
    "        logging.info(f\"{pc_dict}\")\n",
    "        \n",
    "    username_pc_dict[username] = select_biggest(pc_dict)\n",
    "    \n",
    "    logging.info(f\"{username} processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file(os.path.join(in_dir, \"email.csv\"), email_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file(os.path.join(in_dir, \"device.csv\"), device_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file(os.path.join(in_dir, \"file.csv\"), file_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file(os.path.join(in_dir, \"http.csv\"), http_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file(os.path.join(in_dir, \"logon.csv\"), logon_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим соответствие пользователь - комньютер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Генерация предложений пользователей (контекст)\n",
    "Определим список признаков пользователя  \n",
    "Каждая функция будет возвращать список таплов (дата, номер компьютера, контекст, контент)\n",
    "#### общие:\n",
    "- свой/не свой компьютер\n",
    "- за пределами рабочего дня  \n",
    "(2 признака)\n",
    "\n",
    "#### device:\n",
    "- connect\n",
    "- disconnet  \n",
    "(2 признака, контент отсутствует)  \n",
    "\n",
    "#### email:\n",
    "- объем\n",
    "- количество приложений\n",
    "- человек не из компании\n",
    "- контент  \n",
    "(3 признака + контент)\n",
    "\n",
    "### file:\n",
    "(1 вариант + контент)\n",
    "\n",
    "### http:\n",
    "(1 варианта + контент)\n",
    "\n",
    "### logon:\n",
    "- логон\n",
    "- логофф  \n",
    "(2 признака)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     id,date,user,pc,\n",
    "def has_content(filename):\n",
    "    return \"_email\" in filename or \"_file\" in filename or \"_http\" in filename\n",
    "\n",
    "def get_data_pc(line):\n",
    "    t = datetime.strptime(line[1], '%m/%d/%Y %H:%M:%S').time()\n",
    "    assert(line[3][:3] == \"PC-\")\n",
    "    return (t.hour * 60 + t.minute) * 60 + t.second, int(line[3][3:])\n",
    "\n",
    "def device_events(user, date):\n",
    "    #     id,date,user,pc,activity\n",
    "    device_features = []\n",
    "    filename = os.path.join(ds_dir, user, date.strftime('%Y_%m_%d') + \"_device\")\n",
    "    if not os.path.exists(filename):\n",
    "        return []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            features = np.zeros(features_num)\n",
    "            features[int(row[Device.feature_shift + Device.col[\"activity\"]])] = 1\n",
    "            device_features.append((*get_data_pc(row), features, None))\n",
    "    return device_features\n",
    "Device.events = device_events\n",
    "\n",
    "def email_events(user, date):\n",
    "    #     id,date,user,pc,to,cc,bcc,from,size,attachments,content\n",
    "    def check_email(email_list):\n",
    "        emails = email_list.split(';')\n",
    "        for x in emails:\n",
    "            if x not in emails:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    email_features = []\n",
    "    filename = os.path.join(ds_dir, user, date.strftime('%Y_%m_%d') + \"_email\")\n",
    "    if not os.path.exists(filename):\n",
    "        return []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            features = np.zeros(features_num)\n",
    "            features[Email.feature_shift] = int(row[Email.col[\"size\"]])\n",
    "            features[Email.feature_shift+1] = int(row[Email.col[\"attachments\"]])\n",
    "            features[Email.feature_shift+2] = check_email(row[Email.col[\"to\"]])\n",
    "            features[Email.feature_shift+3] = check_email(row[Email.col[\"cc\"]])\n",
    "            email_features.append((*get_data_pc(row), features, row[Email.col[\"content\"]]))\n",
    "    return email_features\n",
    "\n",
    "\n",
    "def file_events(user, date):\n",
    "#     id,date,user,pc,filename,content\n",
    "    email_features = []\n",
    "    filename = os.path.join(ds_dir, user, date.strftime('%Y_%m_%d') + \"_file\")\n",
    "    if not os.path.exists(filename):\n",
    "        return []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            features = np.one(features_num)\n",
    "            email_features.append((*get_data_pc(row), features, row[File.col[\"content\"]]))\n",
    "    return email_features\n",
    "\n",
    "\n",
    "def http_events(user, date):\n",
    "#     id,date,user,pc,url,content\n",
    "    http_features = []\n",
    "    filename = os.path.join(ds_dir, user, date.strftime('%Y_%m_%d') + \"_http\")\n",
    "    if not os.path.exists(filename):\n",
    "        return []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            features = np.ones(features_num)\n",
    "            http_features.append((*get_data_pc(row), features, row[Http.col[\"url\"]].replace(\"/\", \" \").replace(\"_\", \" \") + \" \" + row[Http.col[\"content\"]]))\n",
    "    return http_features\n",
    "\n",
    "\n",
    "def logon_events(user, date):\n",
    "    #     id, date, user, pc, activity\n",
    "    logon_features = []\n",
    "    filename = os.path.join(ds_dir, user, date.strftime('%Y_%m_%d') + \"_logon\")\n",
    "    if not os.path.exists(filename):\n",
    "        return []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            features = np.zeros(features_num)\n",
    "            features[Logon.feature_shift + int(row[Logon.col[\"activity\"]])] = 1\n",
    "            logon_features.append((*get_data_pc(row), features, None))\n",
    "    return logon_features\n",
    "\n",
    "\n",
    "def merge_features(user, *args):\n",
    "    all_data = list(itertools.chain(*args))\n",
    "    all_data.sort(key=lambda x: x[0])\n",
    "    my_computer = np.array([int(t[1] in username_pc_dict[user]) for t in all_data])\n",
    "    return [t[0]//600 for t in all_data], np.hstack((my_computer[..., None], np.vstack([t[2] for t in all_data]))), [t[3] for t in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'AAE0190'\n",
    "date = datetime(2010, 1, 5)\n",
    "features = []\n",
    "for f in (device_events, email_events, file_events, http_events, logon_events):\n",
    "    features.append(f(user, date))\n",
    "    print(features[-1][:3])\n",
    "    print(\"---\")\n",
    "merged = merge_features(user, *features)\n",
    "merged[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка контента\n",
    "Используется FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def sent_gen(max_count=-1):\n",
    "    for d, _, files in tqdm(os.walk(ds_dir)):\n",
    "        for filename in filter(has_content, files):\n",
    "            merged_set = \"\"\n",
    "            with open(os.path.join(d, filename), \"r\") as f:\n",
    "                reader = csv.reader(f, delimiter=',')\n",
    "                for row in reader:\n",
    "                    merged_set = merged_set + \" \" + row[-1]\n",
    "            yield merged_set\n",
    "        max_count -= 1\n",
    "        if max_count == 0:\n",
    "            break \n",
    "            \n",
    "vectorizer = CountVectorizer(min_df=5, max_df=0.9, max_features=5000)                    \n",
    "X = vectorizer.fit_transform(sent_gen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vectorizer_filename, 'wb') as fout:\n",
    "    pickle.dump(vectorizer, fout)\n",
    "with open()\n",
    "np.save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "NetConfig = namedtuple('NetConfig', ['operation_len', 'content_len', 'lin0_size', 'encoder_dropout_rate', 'pos_dropout_rate', 'nhead', 'nlayers', 'nhid'])\n",
    "default_config = NetConfig(\n",
    "    operation_len = merged[1].shape[1],\n",
    "    content_len = fasttext_vec_len, \n",
    "    lin0_size = 32,\n",
    "    encoder_dropout_rate = 0.4, \n",
    "    pos_dropout_rate = 0.1,\n",
    "    nhead = 2, \n",
    "    dlayers = 4, \n",
    "    nhid = 32\n",
    ")\n",
    "\n",
    "\n",
    "class MyBERTModel(nn.Module):\n",
    "    def __init__(self, config):#, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self._linear0 = nn.Linear(config.operation_len + config.content_len, config.lin0_size)\n",
    "        self._pos_encoder = PositionalEncoding(config.lin0_size, config.dropout_rate)\n",
    "        self._layer_nm = nn.LayerNorm(config.lin0_size)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(config.lin0_size, config.nhead, config.nhid, dropout_rate)\n",
    "        self._transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, operations, content, src_mask, timestamps=None):\n",
    "        x = torch.cat((operations, operations), 1)\n",
    "        x = self._linear0(x)\n",
    "        x = self._pos_encoder(x)\n",
    "        output = self.transformer_encoder(x, src_mask)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, config, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self._dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self._dropout(x)\n",
    "    \n",
    "    \n",
    "class ClassifyModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ClassifyModel, self).__init__()\n",
    "        self._BERT = BERTmodel(config)\n",
    "        self._ln = nn.Linear(config.lin0_size, 2)\n",
    "        self._softmax = nn.Softmax(dim=1)        \n",
    "    \n",
    "    def forward(*args):\n",
    "        x = self._BERT(*args)[0, :]\n",
    "        x = self._ln(x)\n",
    "        x = self._softmax(x)\n",
    "        return x\n",
    "    \n",
    "class MaskedPredicter(nn.Model):\n",
    "    def __init__(self, config):\n",
    "        super(MaskedPredicter, self).__init__()\n",
    "        self._BERT = BERTmodel(config)\n",
    "        self._ln = nn.Linear(config.lin0_size, config.operation_len + config.content_len) \n",
    "    \n",
    "    def forward(*args):\n",
    "        x = self._BERT(*args)\n",
    "        x = self._ln(x)\n",
    "        return x\n",
    "    \n",
    "classify_criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
