{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Эксперименты по магистерской диссертации\n",
    "Тема - обнаружение инсайдеров  \n",
    "Датасеты - CERT 4.2, 6.2, в виде БД\n",
    "### План работы:  \n",
    "0) Привести набор данных к удобному виду  \n",
    "1) Предобработка данных (генерация \"предложений поведения пользователя\")  \n",
    "2) Предобработка контентных данных. Пока только письма  \n",
    "3) Обучение трансформера типа Bert поведению пользователей  \n",
    "4) Обучение классификатора пользователей  \n",
    "5) Скор аномальности как ошибка классификатора  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "import csv\n",
    "import logging\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "import math\n",
    "\n",
    "class Device:\n",
    "#     id,date,user,pc,file_tree,activity\n",
    "    col = {name:idx for idx, name in enumerate([\"id\",\"date\",\"user\",\"pc\",\"file_tree\",\"activity\"])}\n",
    "    conncect = 1\n",
    "    disconnect = 0\n",
    "    feature_len = 2\n",
    "    \n",
    "class Email:\n",
    "#     id,date,user,pc,to,cc,bcc,from,activity,size,attachments,content\n",
    "    col = {name:idx for idx, name in enumerate([\"id\",\"date\",\"user\",\"pc\",\"to\",\"cc\",\"bcc\",\"from\",\"activity\",\"size\",\"attachments\",\"content\"])}\n",
    "    view = 0\n",
    "    sent = 1\n",
    "    feature_len = 6\n",
    "    \n",
    "class File:\n",
    "#     id,date,user,pc,filename,activity,to_removable_media,from_removable_media,content\n",
    "    col = {name:idx for idx, name in enumerate([\"id\",\"date\",\"user\",\"pc\",\"filename\",\"activity\",\"to_removable_media\",\"from_removable_media\",\"content\"])}\n",
    "    open = 0\n",
    "    write = 1\n",
    "    copy = 2\n",
    "    delete = 3\n",
    "    feature_len = 6\n",
    "    \n",
    "    \n",
    "class Http:\n",
    "#     id,date,user,pc,url,activity,content\n",
    "    col = {name:idx for idx, name in enumerate([\"id\",\"date\",\"user\",\"pc\",\"url\",\"activity\",\"content\"])}\n",
    "    visit = 0\n",
    "    download = 1\n",
    "    upload = 2\n",
    "    feature_len = 3\n",
    "    \n",
    "    \n",
    "class Logon:\n",
    "#     id, date, user, pc, activity\n",
    "    col = {name:idx for idx, name in enumerate([\"id\",\"date\",\"user\",\"pc\",\"activity\"])}\n",
    "    logon = 1\n",
    "    logout = 0\n",
    "    feature_len = 2\n",
    "    \n",
    "features_num = 0\n",
    "for c in [Device, Email, File, Http, Logon]:\n",
    "    c.feature_shift = features_num\n",
    "    features_num += c.feature_len\n",
    "    \n",
    "ds_dir = \"/media/alina/Elements/datasets/simple_ds\"\n",
    "fasttext_model_path = \"./fast_text_model\"\n",
    "fasttext_vec_len = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "File.feature_shift\n",
    "user_computer = {'AAP0352':{6708}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Привести набор данных к удобному виду\n",
    "Пусть каждому пользователю отвечает директория, а файлы data_lig_type в ней отвечают логу пользователя в конкретный день"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"/media/alina/Elements/datasets/simple_ds\"\n",
    "in_dir = \"/media/alina/65FC89D8465C3712/Documents/универ/курсовая_мага/датасеты/r6.2/\"\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id,date,user,pc\n",
    "def parse_file(filename, function):\n",
    "    log_type = os.path.splitext(os.path.basename(filename))[0]\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        next(reader, None)\n",
    "        user_records = dict()\n",
    "        date = ''\n",
    "        for idx, row in enumerate(reader):\n",
    "            user = row[2]\n",
    "            cur_date = datetime.strptime(row[1], '%m/%d/%Y %H:%M:%S').strftime('%Y_%m_%d')\n",
    "            function(row)\n",
    "            \n",
    "            if cur_date != date:\n",
    "                for u in user_records:\n",
    "                    with open(os.path.join(out_dir, u, date +\"_\"+ log_type), \"w\") as small_f:\n",
    "                        small_f.write('\\n'.join(user_records[u]))\n",
    "                date = cur_date\n",
    "                user_records = dict()\n",
    "                if date[-1] == '0':\n",
    "                    print(f\"{date} parsed\")\n",
    "                \n",
    "            user_records.setdefault(user, [])\n",
    "            user_records[user].append(','.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_f(row):\n",
    "    row[5] = str(1 if row[5] == \"Connect\" else 0)\n",
    "    \n",
    "def email_f(row):\n",
    "    row[8] = str(1 if row[8] == \"Send\" else 0)\n",
    "    row[10] = str(len(row[10].split(';')))\n",
    "    \n",
    "def file_f(row):\n",
    "    activity = File.write\n",
    "    if row[5] == \"File Open\":\n",
    "        activity = File.open\n",
    "    elif row[5] == \"File Copy\":\n",
    "        activity = File.copy\n",
    "    elif row[5] == \"File Delete\":\n",
    "        activity = File.delete\n",
    "    row[5] = str(activity)\n",
    "    row[6] = str(1 if row[6] == \"True\" else 0)\n",
    "    row[7] = str(1 if row[7] == \"True\" else 0)\n",
    "    \n",
    "def http_f(row):\n",
    "    activity = Http.visit\n",
    "    if row[5] == \"WWW Visit\":\n",
    "        activity = Http.visit\n",
    "    elif row[5] == \"WWW Upload\":\n",
    "        activity = Http.upload\n",
    "    elif row[5] == \"WWW Download\":\n",
    "        activity = Http.download\n",
    "    row[5] = str(activity)\n",
    "    \n",
    "def logon_f(row):\n",
    "    row[4] = str(1 if row[4] == \"Logon\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file(os.path.join(in_dir, \"email.csv\"), email_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file(os.path.join(in_dir, \"device.csv\"), device_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file(os.path.join(in_dir, \"file.csv\"), file_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file(os.path.join(in_dir, \"http.csv\"), http_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file(os.path.join(in_dir, \"logon.csv\"), logon_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим соответствие пользователь - комньютер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Генерация предложений пользователей (контекст)\n",
    "Определим список признаков пользователя  \n",
    "Каждая функция будет возвращать список таплов (дата, номер компьютера, контекст, контент)\n",
    "#### общие:\n",
    "- свой/не свой компьютер\n",
    "- за пределами рабочего дня  \n",
    "(2 признака)\n",
    "\n",
    "#### device:\n",
    "- connect\n",
    "- disconnet  \n",
    "(2 признака, контент отсутствует)  \n",
    "\n",
    "#### email:\n",
    "- отсылка\n",
    "- посмотреть письмо\n",
    "- объем\n",
    "- количество приложений\n",
    "- человек не из компании\n",
    "- контент  \n",
    "(3 признака + контент)\n",
    "\n",
    "### file:\n",
    "- действие (4 варианта)\n",
    "- to_removable_media\n",
    "- from_removable_media\n",
    "(5 вариантов + контент)\n",
    "\n",
    "### http:\n",
    "- действие (3 варианта)\n",
    "(3 варианта + контент)\n",
    "\n",
    "### logon:\n",
    "- логон\n",
    "- логофф  \n",
    "(2 признака)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     id,date,user,pc,\n",
    "def has_content(filename):\n",
    "    return \"_email\" in filename or \"_file\" in filename or \"_http\" in filename\n",
    "\n",
    "def get_data_pc(line):\n",
    "    t = datetime.strptime(line[1], '%m/%d/%Y %H:%M:%S').time()\n",
    "    assert(line[3][:3] == \"PC-\")\n",
    "    return (t.hour * 60 + t.minute) * 60 + t.second, int(line[3][3:])\n",
    "\n",
    "def device_events(user, date):\n",
    "    device_features = []\n",
    "    filename = os.path.join(ds_dir, user, date.strftime('%Y_%m_%d') + \"_device\")\n",
    "    if not os.path.exists(filename):\n",
    "        return []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            features = np.zeros(features_num)\n",
    "            features[int(row[Device.feature_shift + Device.col[\"activity\"]])] = 1\n",
    "            device_features.append((*get_data_pc(row), features, None))\n",
    "    return device_features\n",
    "Device.events = device_events\n",
    "\n",
    "def email_events(user, date):\n",
    "    #     id,date,user,pc,to,cc,bcc,from,activity,size,attachments,content\n",
    "    email_features = []\n",
    "    filename = os.path.join(ds_dir, user, date.strftime('%Y_%m_%d') + \"_email\")\n",
    "    if not os.path.exists(filename):\n",
    "        return []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            features = np.zeros(features_num)\n",
    "            features[int(row[Email.feature_shift + Email.col[\"activity\"]])] = 1 # view or send\n",
    "            features[Email.feature_shift + 2] = int(row[Email.col[\"size\"]])\n",
    "            features[Email.feature_shift + 3] = int(row[Email.col[\"attachments\"]])\n",
    "            features[Email.feature_shift + 4] = 0 # TODO \n",
    "            features[Email.feature_shift + 5] = 0 # TODO\n",
    "            email_features.append((*get_data_pc(row), features, row[Email.col[\"content\"]]))\n",
    "    return email_features\n",
    "\n",
    "\n",
    "def file_events(user, date):\n",
    "    #     id,date,user,pc,filename,activity,to_removable_media,from_removable_media,content\n",
    "    email_features = []\n",
    "    filename = os.path.join(ds_dir, user, date.strftime('%Y_%m_%d') + \"_file\")\n",
    "    if not os.path.exists(filename):\n",
    "        return []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            features = np.zeros(features_num)\n",
    "            features[File.feature_shift + int(row[File.col[\"activity\"]])] = 1 # 0...3\n",
    "            features[File.feature_shift + 4] = int(row[File.col[\"to_removable_media\"]]) \n",
    "            features[File.feature_shift + 5] = int(row[File.col[\"from_removable_media\"]]) \n",
    "            email_features.append((*get_data_pc(row), features, row[File.col[\"content\"]]))\n",
    "    return email_features\n",
    "\n",
    "\n",
    "def http_events(user, date):\n",
    "    #     id,date,user,pc,url,activity,content\n",
    "    http_features = []\n",
    "    filename = os.path.join(ds_dir, user, date.strftime('%Y_%m_%d') + \"_http\")\n",
    "    if not os.path.exists(filename):\n",
    "        return []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            features = np.zeros(features_num)\n",
    "            features[Http.feature_shift + int(row[Http.col[\"activity\"]])] = 1\n",
    "            http_features.append((*get_data_pc(row), features, row[Http.col[\"url\"]].replace(\"/\", \" \").replace(\"_\", \" \") + \" \" + row[Http.col[\"content\"]]))\n",
    "    return http_features\n",
    "\n",
    "\n",
    "def logon_events(user, date):\n",
    "    #     id, date, user, pc, activity\n",
    "    logon_features = []\n",
    "    filename = os.path.join(ds_dir, user, date.strftime('%Y_%m_%d') + \"_logon\")\n",
    "    if not os.path.exists(filename):\n",
    "        return []\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            features = np.zeros(features_num)\n",
    "            features[Logon.feature_shift + int(row[Logon.col[\"activity\"]])] = 1\n",
    "            logon_features.append((*get_data_pc(row), features, None))\n",
    "    return logon_features\n",
    "\n",
    "\n",
    "def merge_features(user, *args):\n",
    "    all_data = list(itertools.chain(*args))\n",
    "    all_data.sort(key=lambda x: x[0])\n",
    "    my_computer = np.array([int(t[1] in user_computer[user]) for t in all_data])\n",
    "    return [t[0]//600 for t in all_data], np.hstack((my_computer[..., None], np.vstack([t[2] for t in all_data]))), [t[3] for t in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(29531, 6708, array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), None), (31754, 6708, array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), None), (55824, 6708, array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), None)]\n",
      "---\n",
      "[(39165, 6708, array([0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 2.8293e+04,\n",
      "       1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]), 'It was expected that the building would serve a dual purpose as a museum and as the presidential meeting place for state visitors. Besides architecture'), (39489, 6708, array([0.000000e+00, 0.000000e+00, 0.000000e+00, 1.000000e+00,\n",
      "       3.475206e+06, 3.000000e+00, 0.000000e+00, 0.000000e+00,\n",
      "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
      "       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
      "       0.000000e+00, 0.000000e+00, 0.000000e+00]), 'Photographs of Mount Cayley were taken by Fyles during the 1928 expedition and were published in the 1931 Canadian Alpine Journal Vol XX. Pillow lava is abundant along the bases the flows'), (41425, 6708, array([0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 2.8158e+04,\n",
      "       1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]), 'A portion of the southwestern flank of Mount Fee comprises no volcanic glass')]\n",
      "---\n",
      "[(39489, 6708, array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), 'D0-CF-11-E0-A1-B1-1A-E1 Since his death'), (57066, 6708, array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0.]), 'D0-CF-11-E0-A1-B1-1A-E1 Although the plan was abandoned')]\n",
      "---\n",
      "[(28570, 6708, array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0.]), 'http:  discovercard.com Brian Eaton eaton Cresrpg Qnexarjf79453780.html When Muhammad responded in the latter'), (28703, 6708, array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0.]), 'http:  pch.com Beagle honeywood ubyvqnlf1861283893.php File:Bundesarchiv Bild 192-015'), (28845, 6708, array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0.]), 'http:  people.com Coenwulf of Mercia wulfred fhcreobjy1529142877.php This indicates that it might have developed as part of a subvolcanic intrusion. Because of these concerns')]\n",
      "---\n",
      "[(27900, 6708, array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 1.]), None), (60600, 6708, array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 0.]), None)]\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(102, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = 'AAP0352'\n",
    "date = datetime(2010, 1, 5)\n",
    "features = []\n",
    "for f in (device_events, email_events, file_events, http_events, logon_events):\n",
    "    features.append(f(user, date))\n",
    "    print(features[-1][:3])\n",
    "    print(\"---\")\n",
    "merged = merge_features(user, *features)\n",
    "merged[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка контента\n",
    "Используется FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.parsing.preprocessing  import preprocess_string, preprocess_documents\n",
    "\n",
    "def sent_gen(max_count=-1):\n",
    "    for d, _, files in tqdm(os.walk(ds_dir)):\n",
    "        for filename in filter(has_content, files):\n",
    "            with open(os.path.join(d, filename), \"r\") as f:\n",
    "                reader = csv.reader(f, delimiter=',')\n",
    "                sents = preprocess_documents((row[-1].replace(\"/\", \" \").replace(\"_\", \" \") for row in reader))\n",
    "                for s in sents:\n",
    "                    yield s\n",
    "        max_count -= 1\n",
    "        if max_count == 0:\n",
    "            break \n",
    "# for idx, t in enumerate(sent_gen()):\n",
    "#     print(t)\n",
    "#     if idx == 10:\n",
    "#         break\n",
    "                    \n",
    "model4 = FastText(size=fasttext_vec_len, window=3, min_count=1)\n",
    "model4.build_vocab(sentences = sent_gen(max_count = 2))\n",
    "model4.save(fasttext_model_path)\n",
    "total_examples = model4.corpus_count\n",
    "model4.train(sentences=sent_gen(), total_examples=total_examples, epochs=5)\n",
    "\n",
    "model4.save(fasttext_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.save(fasttext_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "NetConfig = namedtuple('NetConfig', ['operation_len', 'content_len', 'lin0_size', 'encoder_dropout_rate', 'pos_dropout_rate', 'nhead', 'nlayers', 'nhid'])\n",
    "default_config = NetConfig(\n",
    "    operation_len = merged[1].shape[1],\n",
    "    content_len = fasttext_vec_len, \n",
    "    lin0_size = 32,\n",
    "    encoder_dropout_rate = 0.4, \n",
    "    pos_dropout_rate = 0.1,\n",
    "    nhead = 2, \n",
    "    dlayers = 4, \n",
    "    nhid = 32\n",
    ")\n",
    "\n",
    "\n",
    "class MyBERTModel(nn.Module):\n",
    "    def __init__(self, config):#, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self._linear0 = nn.Linear(config.operation_len + config.content_len, config.lin0_size)\n",
    "        self._pos_encoder = PositionalEncoding(config.lin0_size, config.dropout_rate)\n",
    "        self._layer_nm = nn.LayerNorm(config.lin0_size)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(config.lin0_size, config.nhead, config.nhid, dropout_rate)\n",
    "        self._transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, operations, content, src_mask, timestamps=None):\n",
    "        x = torch.cat((operations, operations), 1)\n",
    "        x = self._linear0(x)\n",
    "        x = self._pos_encoder(x)\n",
    "        output = self.transformer_encoder(x, src_mask)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, config, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self._dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self._dropout(x)\n",
    "    \n",
    "    \n",
    "class ClassifyModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self._BERT = BERTmodel(config)\n",
    "        self._ln = nn.Linear(config.lin0_size, 2)\n",
    "        self._softmax = nn.Softmax(dim=1)        \n",
    "    \n",
    "    def forward(*args):\n",
    "        x = self._BERT(*args)\n",
    "        x = self._ln(x)\n",
    "        x = self._softmax(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "classify_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
